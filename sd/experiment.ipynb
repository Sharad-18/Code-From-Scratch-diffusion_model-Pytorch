{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "nn.Linear(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4124,  1.1305, -1.2946,  0.4173, -0.8668]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4871, -0.4604, -0.0471,  0.4587,  0.1042, -0.0338, -0.7203, -0.4314,\n",
       "         -0.6358, -1.1337]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Create a linear layer that takes 5 input features and outputs 6 features\n",
    "linear_layer = nn.Linear(5, 10)\n",
    "\n",
    "# Sample input\n",
    "input_tensor = torch.randn(1, 5)  # Batch size of 1, 5 input features\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "print(input_tensor)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0286e-38]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.empty(2, 3)\n",
    "print(input)\n",
    "torch.ones_like(input,dtype=torch.bool).triu(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.14369045  0.9591966  -0.2701075  -1.1230993  -0.3981025 ]], shape=(1, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.36600307 -0.27534544 -0.49854833  0.52141446 -0.31856024  0.6927415\n",
      "  -0.07178019  0.3842702  -0.0477876  -0.03145728]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create a dense layer with 6 output units\n",
    "dense_layer = tf.keras.layers.Dense(10)\n",
    "\n",
    "# Sample input\n",
    "input_tensor = tf.random.normal((1, 5))  # Batch size of 1, 5 input features\n",
    "output_tensor = dense_layer(input_tensor)\n",
    "print(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = MyModel()\n",
    "input_tensor = torch.randn(1, 10)\n",
    "output = model(input_tensor)  # Calls forward internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9293, -1.2216,  0.5267,  0.4138, -0.4850]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[0.9049709 , 1.2881733 , 0.59506154, 2.1687467 , 0.7042613 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(5)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = MyModel()\n",
    "input_tensor = tf.random.normal([1, 10])\n",
    "output = model(input_tensor)  # Calls call internally\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data\n",
    "inputs = torch.randn(32, 10)\n",
    "labels = torch.randn(32, 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    outputs = model(inputs)  # Forward pass\n",
    "    loss = criterion(outputs, labels)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SimpleModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(50)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.fc2 = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Dummy data\n",
    "inputs = tf.random.normal([32, 10])\n",
    "labels = tf.random.normal([32, 1])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    with tf.GradientTape() as tape:  # Start tracking gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = loss_fn(labels, outputs)  # Compute loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)  # Backward pass\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Update weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 2.4321 - mse: 2.4321\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3538 - mse: 2.3538\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2805 - mse: 2.2805\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2116 - mse: 2.2116\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1469 - mse: 2.1469\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0861 - mse: 2.0861\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0289 - mse: 2.0289\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9751 - mse: 1.9751\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9245 - mse: 1.9245\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8769 - mse: 1.8769\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8320 - mse: 1.8320\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.7897 - mse: 1.7897\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.7499 - mse: 1.7499\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.7122 - mse: 1.7122\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6767 - mse: 1.6767\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6431 - mse: 1.6431\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6114 - mse: 1.6114\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5814 - mse: 1.5814\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.5530 - mse: 1.5530\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5262 - mse: 1.5262\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.5007 - mse: 1.5007\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4766 - mse: 1.4766\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.4537 - mse: 1.4537\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4320 - mse: 1.4320\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4113 - mse: 1.4113\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3917 - mse: 1.3917\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3731 - mse: 1.3731\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.3554 - mse: 1.3554\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3385 - mse: 1.3385\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3224 - mse: 1.3224\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.3071 - mse: 1.3071\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2925 - mse: 1.2925\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.2786 - mse: 1.2786\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2653 - mse: 1.2653\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2526 - mse: 1.2526\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2404 - mse: 1.2404\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2288 - mse: 1.2288\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.2177 - mse: 1.2177\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.2070 - mse: 1.2070\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1968 - mse: 1.1968\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1870 - mse: 1.1870\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1777 - mse: 1.1777\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1687 - mse: 1.1687\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1600 - mse: 1.1600\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1517 - mse: 1.1517\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1437 - mse: 1.1437\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1361 - mse: 1.1361\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1287 - mse: 1.1287\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1216 - mse: 1.1216\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.1148 - mse: 1.1148\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.1082 - mse: 1.1082\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.1018 - mse: 1.1018\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0957 - mse: 1.0957\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0898 - mse: 1.0898\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0841 - mse: 1.0841\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0786 - mse: 1.0786\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0733 - mse: 1.0733\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0681 - mse: 1.0681\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0632 - mse: 1.0632\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0584 - mse: 1.0584\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0537 - mse: 1.0537\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0492 - mse: 1.0492\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0449 - mse: 1.0449\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0407 - mse: 1.0407\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0366 - mse: 1.0366\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0327 - mse: 1.0327\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0288 - mse: 1.0288\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0251 - mse: 1.0251\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0215 - mse: 1.0215\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0180 - mse: 1.0180\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0146 - mse: 1.0146\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0113 - mse: 1.0113\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0081 - mse: 1.0081\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0050 - mse: 1.0050\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0020 - mse: 1.0020\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9991 - mse: 0.9991\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9963 - mse: 0.9963\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9935 - mse: 0.9935\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9908 - mse: 0.9908\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9882 - mse: 0.9882\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9856 - mse: 0.9856\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9832 - mse: 0.9832\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9808 - mse: 0.9808\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9784 - mse: 0.9784\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9761 - mse: 0.9761\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9739 - mse: 0.9739\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9717 - mse: 0.9717\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9696 - mse: 0.9696\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9676 - mse: 0.9676\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9656 - mse: 0.9656\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9636 - mse: 0.9636\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9617 - mse: 0.9617\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9599 - mse: 0.9599\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9581 - mse: 0.9581\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9563 - mse: 0.9563\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9546 - mse: 0.9546\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9529 - mse: 0.9529\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9513 - mse: 0.9513\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9497 - mse: 0.9497\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9482 - mse: 0.9482\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Compile the model (specify loss function, optimizer, and metrics)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss='mse',  # Mean Squared Error\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Dummy data\n",
    "inputs = tf.random.normal([32, 10])\n",
    "labels = tf.random.normal([32, 1])\n",
    "\n",
    "# Training using fit\n",
    "history = model.fit(inputs, labels, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.9704\n",
      "Epoch [2/100], Loss: 0.9611\n",
      "Epoch [3/100], Loss: 0.9522\n",
      "Epoch [4/100], Loss: 0.9436\n",
      "Epoch [5/100], Loss: 0.9354\n",
      "Epoch [6/100], Loss: 0.9274\n",
      "Epoch [7/100], Loss: 0.9197\n",
      "Epoch [8/100], Loss: 0.9122\n",
      "Epoch [9/100], Loss: 0.9049\n",
      "Epoch [10/100], Loss: 0.8978\n",
      "Epoch [11/100], Loss: 0.8909\n",
      "Epoch [12/100], Loss: 0.8842\n",
      "Epoch [13/100], Loss: 0.8777\n",
      "Epoch [14/100], Loss: 0.8712\n",
      "Epoch [15/100], Loss: 0.8649\n",
      "Epoch [16/100], Loss: 0.8588\n",
      "Epoch [17/100], Loss: 0.8527\n",
      "Epoch [18/100], Loss: 0.8468\n",
      "Epoch [19/100], Loss: 0.8410\n",
      "Epoch [20/100], Loss: 0.8353\n",
      "Epoch [21/100], Loss: 0.8297\n",
      "Epoch [22/100], Loss: 0.8242\n",
      "Epoch [23/100], Loss: 0.8188\n",
      "Epoch [24/100], Loss: 0.8135\n",
      "Epoch [25/100], Loss: 0.8082\n",
      "Epoch [26/100], Loss: 0.8031\n",
      "Epoch [27/100], Loss: 0.7980\n",
      "Epoch [28/100], Loss: 0.7930\n",
      "Epoch [29/100], Loss: 0.7880\n",
      "Epoch [30/100], Loss: 0.7831\n",
      "Epoch [31/100], Loss: 0.7782\n",
      "Epoch [32/100], Loss: 0.7734\n",
      "Epoch [33/100], Loss: 0.7688\n",
      "Epoch [34/100], Loss: 0.7641\n",
      "Epoch [35/100], Loss: 0.7595\n",
      "Epoch [36/100], Loss: 0.7550\n",
      "Epoch [37/100], Loss: 0.7505\n",
      "Epoch [38/100], Loss: 0.7460\n",
      "Epoch [39/100], Loss: 0.7416\n",
      "Epoch [40/100], Loss: 0.7373\n",
      "Epoch [41/100], Loss: 0.7329\n",
      "Epoch [42/100], Loss: 0.7287\n",
      "Epoch [43/100], Loss: 0.7244\n",
      "Epoch [44/100], Loss: 0.7202\n",
      "Epoch [45/100], Loss: 0.7160\n",
      "Epoch [46/100], Loss: 0.7119\n",
      "Epoch [47/100], Loss: 0.7078\n",
      "Epoch [48/100], Loss: 0.7037\n",
      "Epoch [49/100], Loss: 0.6996\n",
      "Epoch [50/100], Loss: 0.6956\n",
      "Epoch [51/100], Loss: 0.6916\n",
      "Epoch [52/100], Loss: 0.6877\n",
      "Epoch [53/100], Loss: 0.6837\n",
      "Epoch [54/100], Loss: 0.6798\n",
      "Epoch [55/100], Loss: 0.6760\n",
      "Epoch [56/100], Loss: 0.6721\n",
      "Epoch [57/100], Loss: 0.6683\n",
      "Epoch [58/100], Loss: 0.6645\n",
      "Epoch [59/100], Loss: 0.6608\n",
      "Epoch [60/100], Loss: 0.6570\n",
      "Epoch [61/100], Loss: 0.6533\n",
      "Epoch [62/100], Loss: 0.6496\n",
      "Epoch [63/100], Loss: 0.6459\n",
      "Epoch [64/100], Loss: 0.6423\n",
      "Epoch [65/100], Loss: 0.6387\n",
      "Epoch [66/100], Loss: 0.6350\n",
      "Epoch [67/100], Loss: 0.6315\n",
      "Epoch [68/100], Loss: 0.6279\n",
      "Epoch [69/100], Loss: 0.6243\n",
      "Epoch [70/100], Loss: 0.6208\n",
      "Epoch [71/100], Loss: 0.6173\n",
      "Epoch [72/100], Loss: 0.6138\n",
      "Epoch [73/100], Loss: 0.6104\n",
      "Epoch [74/100], Loss: 0.6069\n",
      "Epoch [75/100], Loss: 0.6035\n",
      "Epoch [76/100], Loss: 0.6001\n",
      "Epoch [77/100], Loss: 0.5967\n",
      "Epoch [78/100], Loss: 0.5934\n",
      "Epoch [79/100], Loss: 0.5900\n",
      "Epoch [80/100], Loss: 0.5867\n",
      "Epoch [81/100], Loss: 0.5834\n",
      "Epoch [82/100], Loss: 0.5801\n",
      "Epoch [83/100], Loss: 0.5769\n",
      "Epoch [84/100], Loss: 0.5737\n",
      "Epoch [85/100], Loss: 0.5704\n",
      "Epoch [86/100], Loss: 0.5673\n",
      "Epoch [87/100], Loss: 0.5641\n",
      "Epoch [88/100], Loss: 0.5609\n",
      "Epoch [89/100], Loss: 0.5577\n",
      "Epoch [90/100], Loss: 0.5546\n",
      "Epoch [91/100], Loss: 0.5515\n",
      "Epoch [92/100], Loss: 0.5484\n",
      "Epoch [93/100], Loss: 0.5453\n",
      "Epoch [94/100], Loss: 0.5423\n",
      "Epoch [95/100], Loss: 0.5393\n",
      "Epoch [96/100], Loss: 0.5363\n",
      "Epoch [97/100], Loss: 0.5333\n",
      "Epoch [98/100], Loss: 0.5303\n",
      "Epoch [99/100], Loss: 0.5273\n",
      "Epoch [100/100], Loss: 0.5244\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a Sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data\n",
    "inputs = torch.randn(32, 10)\n",
    "labels = torch.randn(32, 1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    outputs = model(inputs)  # Forward pass\n",
    "    loss = criterion(outputs, labels)  # Compute loss\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 1.6324 - mse: 1.6324\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.4857 - mse: 1.4857\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3724 - mse: 1.3724\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.2829 - mse: 1.2829\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2106 - mse: 1.2106\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1514 - mse: 1.1514\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1022 - mse: 1.1022\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0608 - mse: 1.0608\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0248 - mse: 1.0248\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9931 - mse: 0.9931\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.9650 - mse: 0.9650\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9395 - mse: 0.9395\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9162 - mse: 0.9162\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8948 - mse: 0.8948\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.8750 - mse: 0.8750\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8566 - mse: 0.8566\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8394 - mse: 0.8394\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8231 - mse: 0.8231\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8078 - mse: 0.8078\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7933 - mse: 0.7933\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7796 - mse: 0.7796\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7666 - mse: 0.7666\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7541 - mse: 0.7541\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7423 - mse: 0.7423\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7309 - mse: 0.7309\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7201 - mse: 0.7201\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7097 - mse: 0.7097\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6997 - mse: 0.6997\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6902 - mse: 0.6902\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6809 - mse: 0.6809\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6721 - mse: 0.6721\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6635 - mse: 0.6635\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6553 - mse: 0.6553\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6474 - mse: 0.6474\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6397 - mse: 0.6397\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6322 - mse: 0.6322\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6250 - mse: 0.6250\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6181 - mse: 0.6181\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6114 - mse: 0.6114\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6049 - mse: 0.6049\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5987 - mse: 0.5987\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5927 - mse: 0.5927\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5869 - mse: 0.5869\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5813 - mse: 0.5813\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5758 - mse: 0.5758\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5706 - mse: 0.5706\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5654 - mse: 0.5654\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5604 - mse: 0.5604\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5556 - mse: 0.5556\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5509 - mse: 0.5509\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5463 - mse: 0.5463\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5418 - mse: 0.5418\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5374 - mse: 0.5374\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5331 - mse: 0.5331\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5289 - mse: 0.5289\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5248 - mse: 0.5248\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5208 - mse: 0.5208\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5170 - mse: 0.5170\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5132 - mse: 0.5132\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5095 - mse: 0.5095\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5059 - mse: 0.5059\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5025 - mse: 0.5025\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4991 - mse: 0.4991\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4958 - mse: 0.4958\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4925 - mse: 0.4925\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4894 - mse: 0.4894\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4863 - mse: 0.4863\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4833 - mse: 0.4833\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4803 - mse: 0.4803\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4774 - mse: 0.4774\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4746 - mse: 0.4746\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4718 - mse: 0.4718\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4691 - mse: 0.4691\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4664 - mse: 0.4664\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4638 - mse: 0.4638\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4612 - mse: 0.4612\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4587 - mse: 0.4587\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4561 - mse: 0.4561\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4537 - mse: 0.4537\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4513 - mse: 0.4513\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4489 - mse: 0.4489\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4466 - mse: 0.4466\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4443 - mse: 0.4443\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4421 - mse: 0.4421\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4398 - mse: 0.4398\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4377 - mse: 0.4377\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4355 - mse: 0.4355\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4334 - mse: 0.4334\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4313 - mse: 0.4313\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4292 - mse: 0.4292\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4271 - mse: 0.4271\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4251 - mse: 0.4251\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4231 - mse: 0.4231\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4211 - mse: 0.4211\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4192 - mse: 0.4192\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4172 - mse: 0.4172\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4153 - mse: 0.4153\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4133 - mse: 0.4133\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4114 - mse: 0.4114\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4095 - mse: 0.4095\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a Sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model (specify loss, optimizer, and metrics)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss='mean_squared_error',  # Loss function\n",
    "              metrics=['mse'])  # Metrics\n",
    "\n",
    "# Dummy data\n",
    "inputs = tf.random.normal([32, 10])\n",
    "labels = tf.random.normal([32, 1])\n",
    "\n",
    "# Train the model using fit\n",
    "history = model.fit(inputs, labels, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder in tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(output_channels, kernel_size=3, padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(output_channels, kernel_size=3, padding='same')\n",
    "        self.activation = tf.keras.layers.ReLU()\n",
    "        \n",
    "        # Skip connection\n",
    "        if input_channels != output_channels:\n",
    "            self.skip = tf.keras.layers.Conv2D(output_channels, kernel_size=1, padding='same')\n",
    "        else:\n",
    "            self.skip = None\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # If skip connection, align dimensions\n",
    "        if self.skip is not None:\n",
    "            residual = self.skip(residual)\n",
    "\n",
    "        return self.activation(x + residual)\n",
    "\n",
    "class VAE_Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        # Layers are defined in the constructor\n",
    "        self.conv1 = tf.keras.layers.Conv2D(128, kernel_size=3, padding='same')\n",
    "        \n",
    "        # Define residual blocks and convolution layers\n",
    "        self.residual1 = ResidualBlock(128, 128)\n",
    "        self.residual2 = ResidualBlock(128, 128)\n",
    "        self.downsample1 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='valid')  # Downsampling\n",
    "        self.residual3 = ResidualBlock(128, 256)\n",
    "        self.residual4 = ResidualBlock(256, 256)\n",
    "        self.downsample2 = tf.keras.layers.Conv2D(256, kernel_size=3, padding='valid')\n",
    "        self.residual5 = ResidualBlock(256, 512)\n",
    "        self.residual6 = ResidualBlock(512, 512)\n",
    "        self.downsample3 = tf.keras.layers.Conv2D(512, kernel_size=3, strides=2, padding='valid')  # Downsampling\n",
    "        \n",
    "        self.residual7 = ResidualBlock(512, 512)\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        self.group_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.silu = tf.keras.layers.Activation('swish')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(8, kernel_size=3, padding='same')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(8, kernel_size=1)\n",
    "\n",
    "    def call(self, x, noise):\n",
    "        # Input image x (batch_size, height, width, channels)\n",
    "        # noise: (batch_size, height/8, width/8, out_channels)\n",
    "        \n",
    "        # Apply each layer sequentially\n",
    "        x = self.conv1(x)  # First convolution, change channels from 3 to 128\n",
    "        x = self.residual1(x)\n",
    "        x = self.residual2(x)\n",
    "        x = self.downsample1(x)  # Downsampling by stride 2\n",
    "        \n",
    "        x = self.residual3(x)\n",
    "        x = self.residual4(x)\n",
    "        x = self.downsample2(x)  # Downsampling again\n",
    "        \n",
    "        x = self.residual5(x)\n",
    "        x = self.residual6(x)\n",
    "        x = self.downsample3(x)  # Final downsampling\n",
    "        \n",
    "        x = self.residual7(x)\n",
    "        \n",
    "        # Attention block (not fully implemented here, but placeholder)\n",
    "        # x = self.attention([x, x])  # Use self-attention\n",
    "        \n",
    "        x = self.group_norm(x)  # Normalization\n",
    "        x = self.silu(x)  # SiLU activation\n",
    "        x = self.conv2(x)  # Reduce channels to 8\n",
    "        x = self.conv3(x)  # Another convolution to refine\n",
    "        \n",
    "        # Split into two tensors: mean and log_variance\n",
    "        mean, log_variance = tf.split(x, num_or_size_splits=2, axis=-1)\n",
    "        log_variance = tf.clip_by_value(log_variance, -30, 28)  # Clamping log_variance\n",
    "        \n",
    "        # Calculate variance and standard deviation\n",
    "        variance = tf.exp(log_variance)\n",
    "        stdev = tf.sqrt(variance)\n",
    "        \n",
    "        # Add noise to the mean and standard deviation\n",
    "        x = mean + stdev * noise\n",
    "        \n",
    "        # Scale the result\n",
    "        x *= 0.18215\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "install_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
